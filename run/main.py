# –° —ç—Ç–∏–º —Ñ–∞–π–ª–æ–º –±—É–¥–µ–º —Ä–∞–±–æ—Ç–∞—Ç—å

import os
import logging
import time
import json
import socket
import openai
import requests
from datetime import datetime, timedelta
import pytz

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è


def _resolve_log_level(value: str | int | None) -> int:
    if isinstance(value, int):
        return value
    if isinstance(value, str):
        name = value.strip().upper()
        candidate = getattr(logging, name, None)
        if isinstance(candidate, int):
            return candidate
        try:
            return int(name)
        except ValueError:
            return logging.INFO
    return logging.INFO


LOG_LEVEL = _resolve_log_level(os.getenv("LOG_LEVEL", "INFO"))

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
SKIP_AI = os.getenv('SKIP_AI', 'false').lower() == 'true'
SKIP_IMAGE = os.getenv('SKIP_IMAGE', 'true').lower() == 'true'
RUN_ON_STARTUP = os.getenv('RUN_ON_STARTUP', 'true').lower() == 'true'
SCHEDULED_HOUR = int(os.getenv('SCHEDULED_HOUR', '2'))
SCHEDULED_MINUTE = int(os.getenv('SCHEDULED_MINUTE', '0'))
TIMEZONE = os.getenv('TIMEZONE', 'Europe/Moscow')

logging.basicConfig(level=LOG_LEVEL, format="%(asctime)s [%(levelname)s] %(message)s")

from _1_google_loader import (
    load_config,
    load_revised_rows,
    load_all_rows,
    update_status_to_published,
    batch_update_cells
)

from _2_content_generation import (
    extract_text_from_url,
    call_openai_assistant,
    call_second_openai_assistant,
    generate_image,
    get_coordinates_from_location,
    translate_title_to_en
)

from _3_create_product import create_product as create_product_en
from _3_create_product import get_category_id_by_name
from _4_create_translation import create_product_pt as create_product_pt
from _5_taxonomy_and_attributes import assign_attributes_to_product
from _6_create_variations import create_variations
from utils import normalize_attribute_payload


def log_network_diagnostics():
    try:
        with open("/etc/resolv.conf", "r", encoding="utf-8") as resolv_file:
            resolv_lines = [line.strip() for line in resolv_file if line.strip()]
        preview = " | ".join(resolv_lines[:5])
        logging.info("üïµÔ∏è DNS resolv.conf: %s", preview)
    except Exception as exc:
        logging.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å /etc/resolv.conf: %s", exc)

    for host in ("oauth2.googleapis.com", "api.opencagedata.com", "api.openai.com"):
        try:
            infos = socket.getaddrinfo(host, None)
            addresses = sorted({info[4][0] for info in infos if info[4]})
            logging.info("üåê DNS %s -> %s", host, ", ".join(addresses))
        except socket.gaierror as exc:
            logging.error("‚ùå DNS %s: %s", host, exc)
        except Exception as exc:  # noqa: BLE001
            logging.warning("‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ DNS –¥–ª—è %s: %s", host, exc)

def collect_all_attributes(variations):
    all_attributes = {}
    for var in variations:
        for attr in var["attributes"]:
            name = attr["name"]
            value = attr["option"]
            if name not in all_attributes:
                all_attributes[name] = set()
            all_attributes[name].add(value)
    return {k: list(v) for k, v in all_attributes.items()}

def get_next_run_time():
    """–í—ã—á–∏—Å–ª—è–µ—Ç –≤—Ä–µ–º—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—É—Å–∫–∞ –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é"""
    moscow_tz = pytz.timezone(TIMEZONE)
    now = datetime.now(moscow_tz)
    
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º—è –∑–∞–ø—É—Å–∫–∞ –Ω–∞ —Å–µ–≥–æ–¥–Ω—è
    scheduled_time = now.replace(hour=SCHEDULED_HOUR, minute=SCHEDULED_MINUTE, second=0, microsecond=0)
    
    # –ï—Å–ª–∏ –≤—Ä–µ–º—è —É–∂–µ –ø—Ä–æ—à–ª–æ —Å–µ–≥–æ–¥–Ω—è, –ø–ª–∞–Ω–∏—Ä—É–µ–º –Ω–∞ –∑–∞–≤—Ç—Ä–∞
    if now >= scheduled_time:
        scheduled_time += timedelta(days=1)
    
    return scheduled_time

def wait_until_next_run():
    """–û–∂–∏–¥–∞–µ—Ç –¥–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—É—Å–∫–∞"""
    next_run = get_next_run_time()
    moscow_tz = pytz.timezone(TIMEZONE)
    now = datetime.now(moscow_tz)
    
    wait_seconds = (next_run - now).total_seconds()
    logging.info(f"‚è∞ –°–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—É—Å–∫ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω –Ω–∞ {next_run.strftime('%Y-%m-%d %H:%M:%S')} –ú–°–ö")
    logging.info(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {wait_seconds:.0f} —Å–µ–∫—É–Ω–¥...")
    
    time.sleep(wait_seconds)

def run_automation():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏"""
    logging.info("üöÄ –ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö")

    log_network_diagnostics()

    config = load_config()

    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –±—ã—Å—Ç—Ä—ã–º–∏ –ø–æ–≤—Ç–æ—Ä–∞–º–∏ –ø—Ä–∏ —Å–µ—Ç–µ–≤—ã—Ö —Å–±–æ—è—Ö
    max_attempts = 3
    delay_sec = 10
    last_error = None
    for attempt in range(1, max_attempts + 1):
        try:
            rows, headers = load_all_rows()
            break
        except Exception as e:
            last_error = e
            logging.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ –∏–∑ Google Sheets (–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_attempts}): {e}")
            if attempt < max_attempts:
                logging.info(f"‚è≥ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ {delay_sec} —Å–µ–∫...")
                time.sleep(delay_sec)
    else:
        # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã ‚Äî –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ, —á—Ç–æ–±—ã –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—ã—à–µ –∏ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –±—ã—Å—Ç—Ä—ã–π –ø–æ–≤—Ç–æ—Ä
        raise last_error

    last_main_row = None
    last_main_row_index = None
    last_main_attributes = {}
    last_variations = []

    for i, (row_index, row) in enumerate(rows):
        status = row.get("STATUS", "").strip().lower()
        row_id = row.get("ID", "unknown")
        
        # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        logging.debug(f"–°—Ç—Ä–æ–∫–∞ {row_index}: ID={row_id}, STATUS='{row.get('STATUS', '')}' -> '{status}'")

        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–æ–∫—É Revised
        if status == "revised":
            logging.info(f"üìå –û–±—Ä–∞–±–æ—Ç–∫–∞ Revised (ID={row.get('ID')})")

            try:
                # --- 1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (GPT + –∫–∞—Ä—Ç–∏–Ω–∫–∞) ---
                lat, lon = get_coordinates_from_location(row.get("LOCATION", ""))
                row["LAT"] = lat if lat is not None else ""
                row["LON"] = lon if lon is not None else ""

                website_text, website_pdf_path = extract_text_from_url(row.get("WEBSITE", ""))
                pt_title = row.get("RACE NAME (PT)", "").strip()
                translated_title = translate_title_to_en(pt_title)
                if translated_title:
                    row["RACE NAME"] = translated_title

                regulations_url = row.get("REGULATIONS", "")
                regulations_text, pdf_path = "", None
                file_ids = []
                if regulations_url:
                    regulations_text, pdf_path = extract_text_from_url(regulations_url)
                    if pdf_path:
                        with open(pdf_path, "rb") as f:
                            upload_response = openai.files.create(file=f, purpose="assistants")
                        file_ids.append(upload_response.id)

                combined_text = ""
                if regulations_url:
                    combined_text += f"\n\nREGULATIONS LINK:\n{regulations_url}"
                combined_text += f"\n\nWEBSITE INFO:\n{website_text}"

                if not combined_text.strip():
                    raise Exception("–ù–µ—Ç —Ç–µ–∫—Å—Ç–∞ –¥–ª—è GPT")

                if SKIP_AI:
                    logging.info("ü§ñ SKIP_AI=true, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫–∏")
                    result = {
                        "summary": "–ó–∞–≥–ª—É—à–∫–∞ summary",
                        "org_info": "–ó–∞–≥–ª—É—à–∫–∞ org_info",
                        "benefits": "–ó–∞–≥–ª—É—à–∫–∞ benefits",
                        "summary_pt": "–ó–∞–≥–ª—É—à–∫–∞ summary_pt",
                        "org_info_pt": "–ó–∞–≥–ª—É—à–∫–∞ org_info_pt",
                        "benefits_pt": "–ó–∞–≥–ª—É—à–∫–∞ benefits_pt",
                        "image_prompt": "Placeholder image"
                    }
                else:
                    # –í—ã–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç
                    first_result = call_openai_assistant(
                        combined_text,
                        file_ids=file_ids
                    )
                    
                    if first_result is None:
                        logging.error("‚ùå –ü–µ—Ä–≤—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                        continue
                    
                    logging.info("‚úÖ –ü–µ—Ä–≤—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É, –ø–µ—Ä–µ–¥–∞—ë–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤–æ –≤—Ç–æ—Ä–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç")
                    
                    # –í—ã–∑—ã–≤–∞–µ–º –≤—Ç–æ—Ä–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –ø–µ—Ä–≤–æ–≥–æ
                    regulations_hint = f"REGULATIONS LINK: {regulations_url if regulations_url else '(empty)'}"
                    result = call_second_openai_assistant(first_result, regulations_hint=regulations_hint)
                    
                    if result is None:
                        logging.error("‚ùå –í—Ç–æ—Ä–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                        continue

                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞—Ä—Ç–∏–Ω–∫–∏
                if SKIP_IMAGE or SKIP_AI:
                    image_info = {"url": "https://dev.racefinder.pt/wp-content/uploads/2025/07/img-placeholder.png", "id": None}
                else:
                    image_info = generate_image(result["image_prompt"])

                row.update({
                    "SUMMARY": result.get("summary", ""),
                    "ORG INFO": result.get("org_info", ""),
                    "BENEFITS": "\n".join(result["benefits"]) if isinstance(result.get("benefits"), list) else result.get("benefits", ""),
                    "IMAGE URL": image_info.get("url", ""),
                    "IMAGE ID": image_info.get("id", ""),
                    "SUMMARY (PT)": result.get("summary_pt", ""),
                    "ORG INFO (PT)": result.get("org_info_pt", ""),
                    "BENEFITS (PT)": "\n".join(result["benefits_pt"]) if isinstance(result.get("benefits_pt"), list) else result.get("benefits_pt", ""),
                    "LAT": row["LAT"],
                    "LON": row["LON"],
                    "RACE NAME (PT)": row.get("RACE NAME (PT)", ""),
                    "image_id": image_info.get("id", None)
                })

                # üì§ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ç–∞–±–ª–∏—Ü—É
                batch_update_cells(row_index, {
                    "SUMMARY": row["SUMMARY"],
                    "ORG INFO": row["ORG INFO"],
                    "BENEFITS": row["BENEFITS"],
                    "IMAGE URL": row["IMAGE URL"],
                    "IMAGE ID": row["IMAGE ID"],
                    "SUMMARY (PT)": row["SUMMARY (PT)"],
                    "ORG INFO (PT)": row["ORG INFO (PT)"],
                    "BENEFITS (PT)": row["BENEFITS (PT)"],
                    "RACE NAME (PT)": row["RACE NAME (PT)"],
                    "RACE NAME": row.get("RACE NAME", "")
                }, headers)

                # --- 2. –°–æ–±–∏—Ä–∞–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã –∏ –ø–µ—Ä–≤—É—é –≤–∞—Ä–∏–∞—Ü–∏—é ---
                last_main_row = row.copy()
                last_main_row_index = row_index
                last_main_attributes = {}
                last_main_row["extra_categories"] = set()
                main_category = row.get("CATEGORY")
                main_subcategory = row.get("SUBCATEGORY")
                if main_category:
                    last_main_row["extra_categories"].add((main_category, main_subcategory))
                if row.get("ATTRIBUTE") and row.get("VALUE"):
                    last_main_attributes[row["ATTRIBUTE"]] = row["VALUE"]

                for attr_name, col in [
                    ("Distance", "DISTANCE"),
                    ("Team", "TEAM"),
                    ("Type", "TYPE"),
                    ("License", "LICENSE"),
                    ("Race Start Date", "RACE START DATE"),
                    ("Race Start Time", "RACE START TIME")
                ]:
                    if row.get(col):
                        last_main_attributes[attr_name] = row[col]

                variation_attributes = [{"name": k, "option": v} for k, v in last_main_attributes.items()]
                last_variations = [{
                    "regular_price": str(row.get("PRICE", "0")),
                    "attributes": variation_attributes
                }]

                # --- 3. –°–æ–±–∏—Ä–∞–µ–º –ø–æ–¥–≤–∞—Ä–∏–∞—Ü–∏–∏ ---
                for j in range(i + 1, len(rows)):
                    sub_index, sub_row = rows[j]
                    sub_status = sub_row.get("STATUS", "").strip().lower()
                    if sub_status in ("revised", "published"):
                        break
                    if sub_status == "":
                        var_attrs = []
                        if sub_row.get("ATTRIBUTE") and sub_row.get("VALUE"):
                            var_attrs.append({"name": sub_row["ATTRIBUTE"], "option": sub_row["VALUE"]})
                        variation_category = sub_row.get("CATEGORY")
                        if variation_category:
                            last_main_row["extra_categories"].add((variation_category, sub_row.get("SUBCATEGORY")))
                        for attr_name, col in [
                            ("Distance", "DISTANCE"),
                            ("Team", "TEAM"),
                            ("Type", "TYPE"),
                            ("License", "LICENSE"),
                            ("Race Start Date", "RACE START DATE"),
                            ("Race Start Time", "RACE START TIME")
                        ]:
                            if sub_row.get(col):
                                var_attrs.append({"name": attr_name, "option": sub_row[col]})
                        if var_attrs:
                            last_variations.append({
                                "regular_price": str(sub_row.get("PRICE", "0")),
                                "attributes": var_attrs
                            })
                    else:
                        break

                # --- 4. –ü—É–±–ª–∏–∫–∞—Ü–∏—è –≤ WooCommerce ---
                lat, lon = get_coordinates_from_location(last_main_row.get("LOCATION", ""))
                last_main_row["LAT"] = lat if lat is not None else ""
                last_main_row["LON"] = lon if lon is not None else ""
                last_main_row["extra_categories"] = [
                    (cat, sub_cat)
                    for cat, sub_cat in last_main_row.get("extra_categories", set())
                    if cat
                ]

                en_product_id = create_product_en(last_main_row)
                last_main_row["en_product_id"] = en_product_id

                # –ü–æ–ª—É—á–∞–µ–º slug
                try:
                    r = requests.get(f"{config['wp_url']}/wp-json/wc/v3/products/{en_product_id}",
                                     auth=(config["consumer_key"], config["consumer_secret"]))
                    r.raise_for_status()
                    data = r.json()
                    slug = data.get("slug", "")
                    permalink = data.get("permalink", "")
                    if permalink:
                        last_main_row["LINK RACEFINDER"] = permalink
                    elif slug:
                        last_main_row["LINK RACEFINDER"] = f"https://dev.racefinder.pt/event/{slug}"
                    else:
                        last_main_row["LINK RACEFINDER"] = ""
                except Exception as e:
                    logging.error(f"Slug error: {e}")
                    last_main_row["LINK RACEFINDER"] = ""

                attr_payload = normalize_attribute_payload(last_main_attributes)
                for var in last_variations:
                    for attr in var["attributes"]:
                        if attr["name"] not in attr_payload:
                            attr_payload[attr["name"]] = []
                        elif not isinstance(attr_payload[attr["name"]], list):
                            attr_payload[attr["name"]] = [attr_payload[attr["name"]]]
                        if attr["option"] not in attr_payload[attr["name"]]:
                            attr_payload[attr["name"]].append(attr["option"])

                assign_attributes_to_product(en_product_id, attr_payload)
                create_variations(en_product_id, last_variations)

                pt_product_id = create_product_pt(
                    last_main_row,
                    en_product_id,
                    attributes=attr_payload,
                    last_variations=last_variations,
                    config=config
                )
                last_main_row["pt_product_id"] = pt_product_id

                # --- 5. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –≤ —Ç–∞–±–ª–∏—Ü–µ ---
                batch_update_cells(last_main_row_index, {
                    "STATUS": "Published",
                    "LINK RACEFINDER": last_main_row.get("LINK RACEFINDER", "")
                }, headers)

                logging.info(f"‚úÖ Published ID={row.get('ID')} EN={en_product_id} PT={pt_product_id}")

            except Exception as e:
                logging.exception(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ Revised ID={row.get('ID')}")
                continue

        elif status == "published":
            logging.debug(f"‚è≠ –ü—Ä–æ–ø—É—Å–∫ Published (ID={row.get('ID')})")
            continue

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ–º"""
    moscow_tz = pytz.timezone(TIMEZONE)
    now = datetime.now(moscow_tz)
    
    logging.info(f"üïê –¢–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è: {now.strftime('%Y-%m-%d %H:%M:%S')} –ú–°–ö")
    logging.info(f"‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏: RUN_ON_STARTUP={RUN_ON_STARTUP}, SCHEDULED_HOUR={SCHEDULED_HOUR}:{SCHEDULED_MINUTE:02d}")
    
    # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
    if RUN_ON_STARTUP:
        logging.info("üöÄ –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞")
        try:
            run_automation()
            logging.info("‚úÖ –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logging.exception("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–æ–≤–æ–º –∑–∞–ø—É—Å–∫–µ")
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Å —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ–º
    while True:
        # –û–∂–∏–¥–∞–Ω–∏–µ –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—É—Å–∫–∞
        wait_until_next_run()

        try:
            # –ó–∞–ø—É—Å–∫ –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é
            logging.info("üîÑ –ó–∞–ø—É—Å–∫ –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é")
            run_automation()
            logging.info("‚úÖ –ó–∞–ø—É—Å–∫ –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")

        except Exception as e:
            logging.exception("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é")
            # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ –Ω–æ—Å–∏—Ç —Å–µ—Ç–µ–≤–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä ‚Äî –¥–µ–ª–∞–µ–º –±—ã—Å—Ç—Ä—ã–π –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ 15 –º–∏–Ω—É—Ç, –Ω–µ –æ—Ç–∫–ª–∞–¥—ã–≤–∞—è –Ω–∞ —Å—É—Ç–∫–∏
            quick_retry_minutes = 15
            logging.info(f"‚è≥ –ë—ã—Å—Ç—Ä—ã–π –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {quick_retry_minutes} –º–∏–Ω—É—Ç...")
            time.sleep(quick_retry_minutes * 60)
            try:
                logging.info("üîÑ –ë—ã—Å—Ç—Ä—ã–π –ø–æ–≤—Ç–æ—Ä –∑–∞–ø—É—Å–∫–∞ –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é")
                run_automation()
                logging.info("‚úÖ –ë—ã—Å—Ç—Ä—ã–π –ø–æ–≤—Ç–æ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            except Exception:
                logging.exception("‚ùå –ë—ã—Å—Ç—Ä—ã–π –ø–æ–≤—Ç–æ—Ä –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –æ—à–∏–±–∫–æ–π. –û–∂–∏–¥–∞–µ–º —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø–ª–∞–Ω–æ–≤–æ–≥–æ –∑–∞–ø—É—Å–∫–∞.")

if __name__ == "__main__":
    main()
